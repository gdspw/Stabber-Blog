---
layout: post
title:  "java基础面试知识点总结-多线程1"
categories: [java,jvm]
tags:  [Java,IT]
excerpt: 总结记录的一些有关java基础知识的面试题

---

## 说一下Java多线程之内存可见性

​    （其实就是java的volatile实现原理，我补充了volatile的禁止指令重排序的原理。这里我扯到了synchronized的JVM底层实现原理，AQS工具包。）

​    首先要明确一点，每个线程都有属于自己的工作内存。除了线程自己拥有的工作内存外，还有公共的物理主内存。如果一个变量在多个线程的工作内存中都存在副本，那么这个变量就是这几个线程的**共享变量**。若其中一个线程对共享变量的修改，能够被其它线程看到，那么就能说明共享变量在线程之间是**可见的**。**为什么会出现共享变量可见性的问题？**这是因为JMM中线程操作内存的两个基本规定：①线程对共享变量的所有操作都必须在自己的工作内存中进行，不能从主内存中读写；②而且不同线程之间无法直接访问其它线程工作内存中的变量，线程间变量值的传递需要通过主内存来完成。

​    Java语言层面支持的可见性实现方式有两种：

n **1****、Synchronized**：不仅能通过互斥锁来实现同步（原子性），而且还能够实现可见性。**线程解锁前对共享变量的修改在下次加锁时对其他线程可见，****JMM****关于synchronized 的两条规定：**

u 线程解锁前，必须把共享变量的最新值刷新到主内存中；

u 线程加锁时，将清空工作内存中共享变量的值，从而使用共享变量时需要从主内存中重新读取最新的值（注意：加锁与解锁需要的是同一把锁）

**线程执行互斥代码的过程：**

1、 获得互斥锁

2、 清空工作内存

3、 从主内存拷贝变量的最新副本到工作内存

4、 执行代码

5、 将更改后的共享变量的值刷新到主内存

6、 释放互斥锁

n **2****、volatile**：通过加入内存屏障和禁止指令重排序优化来实现可见性的，**但不保证原子性**。

u 对volatile变量执行**写**操作时，会在写操作后加入一条store屏障指令，这样就会把读写时的数据缓存加载到主内存中；

u 对volatile变量执行**读**操作时，会在读操作前加入一条load屏障指令，这样就会从主内存中加载变量；

​    所以说，volatile变量在每次被线程访问时，都强迫从主内存中重读该变量的值，而当该变量发生变化时，就会强迫线程将最新的值刷新到主内存，这样任何时刻，不同的线程总能看到该变量的最新值。

## 除了volatile还有什么方法保证数据一致性

volatile的使用场景：

l 状态标志(开关模式) 

l 双重检查锁定

l 需要利用顺序性（防止指令重排序）。

除了volatile在多线程情况下能保证数据一致性，还有以下3种方法：

l 使用synchronized关键字

l 使用Lock锁

l 使用Atomic原子类

## 深入剖析volatile关键字

一旦一个共享变量（类的成员变量、类的静态成员变量）被volatile修饰之后，那么就具备了两层语义：

1）保证了不同线程对这个变量进行操作时的可见性，即一个线程修改了某个变量的值，这新值对其他线程来说是立即可见的。

2）禁止进行指令重排序。volatile关键字禁止指令重排序有两层意思：

　　（A）当程序执行到volatile变量的读操作或者写操作时，在其前面的操作的更改肯定全部已经进行，且结果已经对后面的操作可见；在其后面的操作肯定还没有进行；

　　（B）在进行指令优化时，不能将在对volatile变量访问的语句放在其后面执行，也不能把volatile变量后面的语句放到其前面执行。

volatile 关键字的作用是禁止指令的重排序，强制从公共堆栈（主存）中取得变量的值，而不是从线程私有的数据栈（工作内存）中取变量的值。

先看一段代码，假如线程1先执行，线程2后执行：

```java
// 线程1

boolean stop = false;

while(!stop){

  doSomething();

}

// 线程2

stop = true;
```



 这段代码是很典型的一段代码，很多人在中断线程时可能都会采用这种标记办法。但是事实上，这段代码会完全运行正确么？即一定会将线程中断么？不一定，也许在大多数时候，这个代码能够把线程中断，但是也有可能会导致无法中断线程（虽然这个可能性很小，但是只要一旦发生这种情况就会造成死循环了）。

下面解释一下这段代码为何有可能导致无法中断线程。在前面已经解释过，每个线程在运行过程中都有自己的工作内存，那么线程1在运行的时候，会将stop变量的值拷贝一份放在自己的工作内存当中。

那么当线程2更改了stop变量的值之后，但是还没来得及写入主存当中，线程2转去做其他事情了，那么线程1由于不知道线程2对stop变量的更改，因此还会一直循环下去。

但是用volatile修饰之后就变得不一样了：

第一：使用volatile关键字会强制将修改的值立即写入主存；

第二：使用volatile关键字的话，当线程2进行修改时，会导致线程1的工作内存中缓存变量stop的缓存行无效（反映到硬件层的话，就是CPU的L1或者L2缓存中对应的缓存行无效）；

第三：由于线程1的工作内存中缓存变量stop的缓存行无效，所以线程1再次读取变量stop的值时会去主存读取。

那么在线程2修改stop值时（当然这里包括2个操作，修改线程2工作内存中的值，然后将修改后的值写入内存），会使得线程1的工作内存中缓存变量stop的缓存行无效，然后线程1读取时，发现自己的缓存行无效，它会等待缓存行对应的主存地址被更新之后，然后去对应的主存读取最新的值。那么线程1读取到的就是最新的正确的值。

从上面知道volatile关键字保证了操作的可见性，但是volatile能保证对变量的操作是原子性吗？

​    不能。读**->**存工作内存**->**操作**->**写工作内存**->**写主存。只能synchronized和Lock锁来实现原子性操作的（Compare And Swap），CAS实际上是利用处理器提供的CMPXCHG指令实现的，而处理器执行CMPXCHG指令是一个原子性操作。

**volatile****的原理和实现机制**

前面讲述了源于volatile关键字的一些使用，下面我们来探讨一下volatile到底如何保证可见性和禁止指令重排序的。

下面这段话摘自《深入理解Java虚拟机》：

“观察加入volatile关键字和没有加入volatile关键字时所生成的汇编代码发现，加入volatile关键字时，会多出一个lock前缀指令”

lock前缀指令实际上相当于一个内存屏障（也成内存栅栏），内存屏障会提供3个功能：

1）它确保指令重排序时不会把其后面的指令排到内存屏障之前的位置，也不会把前面的指令排到内存屏障的后面；即在执行到内存屏障这句指令时，在它前面的操作已经全部完成；

2）它会强制将对缓存的修改操作立即写入主存；

3）如果是写操作，它会导致其他CPU中对应的缓存行无效。

使用volatile必须具备以下2个条件：

　　1）对变量的写操作不依赖于当前值

　　2）该变量没有包含在具有其他变量的不变式中

列举几个Java中使用volatile的几个场景：1.状态标记量，**2.double check****。**

## 什么是内存屏障？

l **内存屏障**（Memory Barrier）是一种CPU指令。

​    内存屏障也称为内存栅栏或栅栏指令，是一种屏障指令，它使CPU或编译器对屏障指令之前和之后发出的内存操作执行一个排序约束。目前的高级处理器CPU，为了提高内部逻辑元件的利用率以提高运行速度，通常会采用多指令发射、乱序执行等各种措施。现在普遍使用的一些超标量处理器通常能够在 一个指令周期内并发执行多条指令。**可以解决数据一致性问题：**由于编译器的优化和缓存的使用，导致对内存的写入操作不能及时的反应出来，也就是说当完成对内存的写入操作之后，读取出来的可能是旧的内容。为了防止编译器和硬件的不正确优化，使得对存储器的访问顺序（其实就是变量）和书写程序时的访问顺序不一致而提出的一种解决办法。

​    **内存屏障的分类：**

1、 编译器引起的内存屏障

2、 缓存引起的内存屏障

3、 乱序执行引起的内存屏障

**按功能分类有以下四种：**

   **1****、write（或store）内存屏障**

   保证所有该屏障之前的store操作，看起来一定在所有该屏障之后的store操作之前执行。

   **2****、read（或load）内存屏障**

   保证所有该屏障之前的load操作，看起来一定在所有该屏障之后的load操作之前执行。仅保证load指令上的偏序关系，不要求对store指令有什么影响。

   **3****、数据依赖屏障**

   是read屏障的一种较弱形式。在执行两个load指令，第二个依赖于第一个的执行结果（例如：第一个load执行获取某个地址，第二个load指令取该地址的值）时，可能就需要一个数据依赖屏障，来确保第二个load指令在获取目标地址值的时候，第一个load指令已经更新过该地址。仅保证相互依赖的load指令上的偏序关系，不要求对store指令，无关联的load指令以及重叠的load指令有什么影响。

   **4****、通用内存屏障**

   确保所有该屏障之前的load和store操作，看起来一定在所有屏障之后的load和store操作之前执行。它能保证load和store指令上的偏序关系。

   **5****、一对隐式的屏障变种（LOCK和UNLOCK操作）：**

   LOCK操作可以看作是一个单向渗透的屏障。它保证所有在LOCK之后的内存操作看起来一定在LOCK操作后才发生。

   UNLOCK操作也是一个单向渗透屏障。它保证所有UNLOCK操作之前的内存操作看起来一定在UNLOCK操作之前发生。

 

​    实际运用场景：volatile便是基于内存屏障实现的。可以了解一下Dekker算法中的内存屏障。该算法利用volatile变量协调两个线程之间的共享资源访问。

## 什么是内存对齐？

l **内存对齐：**

​    内存地址对齐，是一种在计算机内存中排列数据（表现为变量的地址）、访问数据（表现为CPU读取数据）的一种方式，包含了两种相互独立又相互关联的部分：基本数据对齐和结构体数据对齐 。

​    为什么需要内存对齐？对齐有什么好处？是我们程序员来手动做内存对齐呢？还是编译器在进行自动优化的时候完成这项工作？

​    在现代计算机体系中，每次读写内存中数据，都是按字（word，4个字节，对于X86架构，系统是32位，数据总线和地址总线的宽度都是32位，所以最大的寻址空间为2的32次方 = 4GB，按A[31,30…2,1,0]这样排列，但是请注意为了CPU每次读写 4个字节寻址，A[0]和A[1]两位是不参与寻址计算的。）为一个块（chunks）来操作（而对于X64则是8个字节为一个快）。注意，这里说的 CPU每次读取的规则，并不是变量在内存中地址对齐规则。既然是这样的，如果变量在内存中存储的时候也按照这样的对齐规则，就可以加快CPU读写内存的速度，当然也就提高了整个程序的性能，并且性能提升是客观，虽然当今的CPU的处理数据速度(是指逻辑运算等,不包括取址)远比内存访问的速度快，程序的执 行速度的瓶颈往往不是CPU的处理速度不够，而是内存访问的延迟，虽然当今CPU中加入了高速缓存用来掩盖内存访问的延迟，但是如果高密集的内存访问，这种延迟是无可避免的，内存地址对齐会给程序带来了很大的性能提升。

  内存地址对齐是计算机语言自动进行的，也即是编译器所做的工作。但这不意味着我们程序员不需要做任何事情，因为如果我们能够遵循某些规则，可以让编译器做得更好，毕竟编译器不是万能的。

**1****、基本数据对齐**

​    在X86，32位系统下基于Microsoft、Borland和GNU的编译器，有如下数据对齐规则：

​    a、一个char（占用1-byte）变量以1-byte对齐。

​    b、一个short（占用2-byte）变量以2-byte对齐。

​    c、一个int（占用4-byte）变量以4-byte对齐。

​    d、一个long（占用4-byte）变量以4-byte对齐。

​    e、一个float（占用4-byte）变量以4-byte对齐。

​    f、一个double（占用8-byte）变量以8-byte对齐。

​    g、一个long double（占用12-byte）变量以4-byte对齐。

​    h、任何pointer（占用4-byte）变量以4-byte对齐。

​    **而在64位系统下，与上面规则对比有如下不同：**

​    a、一个long（占用8-byte）变量以8-byte对齐。

​    b、一个double（占用8-byte）变量以8-byte对齐。

​    c、一个long double（占用16-byte）变量以16-byte对齐。

​    d、任何pointer（占用8-byte）变量以8-byte对齐。

  **2****、结构体数据对齐**

  结构体数据对齐，是指结构体内的各个数据对齐。在结构体中的第一个成员的首地址等于整个结构体的变量的首地址，而后的成员的地址随着它声明的顺序和实际占用的字节数递增。为了总的结构体大小对齐，会在结构体中插入一些没有实际意思的字符来**填充（padding）**结构体。

  在结构体中，成员数据对齐满足以下规则：

  a、结构体中的第一个成员的首地址也即是结构体变量的首地址。

  b、结构体中的每一个成员的首地址相对于结构体的首地址的偏移量（offset）是该成员数据类型大小的整数倍。

  c、结构体的总大小是对齐模数（对齐模数等于#pragma pack(n)所指定的n与结构体中最大数据类型的成员大小的最小值）的整数倍。

## Synchronized锁粒度（使用方法和作用域）

1、修饰**方法**：在范围操作符之后，返回类型声明之前使用。每次只能有一个线程进入该方法，

​    此时线程获得的是**方法锁**。

​    public synchronized void method(){ }

2、修饰**代码块**：每次只能有一个线程进入该代码块，

​       此时线程获得的是（this）**对象锁**。

​    public void method() {synchronized(this) {// dosomething }}

3、修饰**对象**：如果当前线程进入，那么其他线程在该类所有对象上的任何操作都不能进行，

​       此时当前线程获得的是**对象锁**。

​    public void method() {synchronized(objInstance) {// dosomething }}

4、修饰**类**：如果当前线程进入，那么其他线程在该类中所有操作不能进行，包括静态变量和静态方法，

​       此时当前线程获得的是Class**类对象锁**。

   public void method1() {synchronized(MyClass.class){ }}

   public static synchronized void method2(){ }

## Java的synchronized加在静态方法和动态方法的区别？

synchronized修饰静态方法，是对该类对象（XXX.class)加锁，俗称“类锁”。对于静态同步方法，锁是针对这个类的，锁对象是该类的Class对象。静态和非静态方法的锁互不干预。例如：

public static synchronized void method1( ) {  }

public static void method1( ) { synchronized(Xxx.class) {  } }

synchronized修饰非静态方法，是对调用该方法的对象加锁，俗称“对象锁”。静态同步方法和非静态同步方法将永远不会彼此阻塞，因为静态方法锁定在Class对象上，非静态方法锁定在该类的对象上。

public synchronized void method1() {  }

public void method1() { synchronized(this) {  } }

一个线程获得锁，当在一个同步方法中访问另外对象上的同步方法时，会获取这两个对象锁。

## 静态方法跟非静态方法的锁区别

**类锁（又称全局锁，static synchronized）**

​    synchronized修饰静态方法，该锁是对该类Class对象加锁，无论实例出多少个对象，类对象只有一个，那么线程依然共享该锁。和用单例模式声明一个对象来调用非静态方法的情况是一样的，因为永远就只有这一个对象。所以访问同步方法之间一定是互斥的。

​    static synchronized是限制多线程中该类的所有实例同时访问该类所对应的代码块。

**注意：**

1、static synchronized并不是关键字，只是代表给静态方法加锁。

2、锁住的只是static synchronized块，synchronized块锁不住，而不加锁的方法更加锁不住

**对象锁（实例锁，synchronized）**

​    synchronized修饰非静态方法，该锁是对调用该方法的当前实例对象加锁，每个对象有且仅有唯一的1个锁。

​    synchronized是对类的当前实例（当前对象）进行加锁，防止其他线程同时访问该类的该实例的所有synchronized块。

**注意：**

1、指的是“类的当前实例”， 类的两个不同实例就没有这种约束了。

2、锁住的只是synchronized块，static synchronized块锁不住，而不加锁的方法更加锁不住。

**总结:**

·    静态方法的锁属于类, 一个类中所有加锁的静态方法共用该锁

·    非静态方法的锁属于对象, 一个对象中所有加锁的非静态方法共用, 和静态方法的锁不同而互不相干

·    加锁的方法的执行不会影响同一个类/对象中未加锁的方法的执行(因为其他方法没有锁呀)

## 何时需要同步

在多个线程同时并发访问同一个共享资源（全局变量）时，应该使用同步机制（独占方式访问）以保证此资源的数据安全。互斥确保不会同时修改它，即线程安全。要跨线程维护正确的可见性，只要在几个线程之间共享非 final 变量，就必须使用synchronized（或 volatile）以确保一个线程可以看见另一个线程做的更改。为了在线程之间进行可靠的通信，也为了互斥访问，同步是必须的。这归因于java语言规范的内存模型，它规定了：一个线程所做的变化何时以及如何变成对其它线程可见。因为多线程将异步行为引进程序，所以在需要同步时，必须有一种方法强制进行。

Java同步机制有4种实现方式：

**①ThreadLocal:** 线程级别作用域副本，既保证全局变量线程安全，又实现存在当前线程中多次访问全局变量的便利，如JDBC Connection对象，需在线程级别下多个方法多次访问数据库连接。

**②synchronized( )****：**监视器字节码指令monitorenter、monitorexit，重量级锁、悲观锁、内置锁

**③wait()****与notify()**：对象锁

**④volatile****：**内存屏障（Memory Barriers）不允许线程内部缓存（即直接修改内存）和重排序。

**监控器（Monitor）**是一个控制机制，可以认为是一个很小的、只能容纳一个线程的盒子，一旦一个线程进入监控器，其它的线程必须等待，直到那个线程退出监控为止。通过这种方式，一个监控器可以保证共享资源在同一时刻只可被一个线程使用。这种方式称之为同步。

**锁（Lock）**提供了两种主要特性：互斥（mutual exclusion）和可见性（visibility）。**互斥**即一次只允许一个线程持有某个特定的锁，因此可使用该特性实现对共享数据的协调访问协议，这样，一次就只有一个线程能够使用该共享数据。**可见性**要更加复杂一些，它必须确保释放锁之前对共享数据做出的更改对于随后获得该锁的另一个线程是可见的 —— 如果没有同步机制提供的这种可见性保证，线程看到的共享变量可能是修改前的值或不一致的值，造成数据不一致导致计算结果错误。

## 进程与线程的区别

​    **进程：**是并发执行的程序在执行过程中分配和管理资源的基本单位，是一个动态概念，竞争计算机系统资源的基本单位。

​    **线程：**是进程的一个执行单元，是进程内科调度实体，也是处理器调度的基本单位。比进程更小的独立运行的基本单位。线程也被称为轻量级进程。

​    进程是（系统）资源分配最小单位，线程是程序执行的最小单位。关系是进程–>线程–>程序。所以线程是程序执行流的最小单位，而进程是系统进行资源分配和调度的一个独立单位。

**进程与线程的区别：**

·    **地址空间**：同一进程的线程共享本进程的地址空间，而进程之间则是独立的地址空间。

·    **资源拥有**：同一进程内的线程共享本进程的资源如内存、I/O、cpu等，但是进程之间的资源是独立的。线程占用的资源要⽐进程少很多。

·    **执行过程**：每个独立的进程都有一个程序运行的入口、顺序执行序列和程序入口。但是线程不能独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。

 

## 线程状态，以及相互转化的过程

![img](http://image.itstabber.com/2020-09-09/clip_image017.png)

1. **新建(new)**：新创建了一个线程对象，并没有调用start()方法之前。

2. **可运行(runnable)**：线程对象创建后，其他线程(比如main线程）调用了该对象的start()方法。该状态的线程位于可运行线程池中，等待被线程调度选中，获取CPU的使用权。

3. **运行(running)**：可运行状态(runnable)的线程获得了CPU时间片（timeslice），执行程序代码。

4. **阻塞(block)**：阻塞状态是指线程因为某种原因放弃了CPU使用权，也即让出了CPU timeslice，暂时停止运行（线程仍旧是活的）。直到线程进入可运行(runnable)状态，才有机会再次获得CPU timeslice转到运行(running)状态。阻塞的情况分三种：

**(****一). 等待阻塞**：运行(running)的线程执行锁对象的o.wait()方法，JVM会把该线程放入等待队列(waitting queue)中。

**(****二). 同步阻塞**：运行(running)的线程在获取对象的同步锁时，若该同步锁被别的线程占用，则JVM会把该线程放入锁池(**lock** pool)中。

**(****三). 其他阻塞**: 运行(running)的线程执行睡眠Thread.sleep(**long** ms)或合并t.**join**()方法，或者发出了I/O请求时，JVM会把该线程置为阻塞状态。当sleep()状态超时**join**()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入可运行(runnable)状态。

5. **死亡(dead)**：线程run()、main()方法执行结束，或者因异常退出了run()方法，则该线程结束生命周期。死亡的线程不可再次复生。

## 有哪些方式方法可以让线程离开运行状态？

1、调用Thread.sleep()：使当前线程睡眠至少多少毫秒（尽管它可能在指定的时间之前被中断）。

2、调用Thread.yield()：不能保障太多事情，尽管通常它会让当前运行线程回到可运行性状态，使得有相同优先级的线程有机会执行。

3、调用join()方法：保证当前线程停止执行，直到该线程所加入的线程完成为止。然而，如果它加入的线程没有存活，则当前线程不需要停止。

除了以上三种方式外，还有下面几种特殊情况可能使线程离开运行状态：

1、线程的run()方法完成。

2、在持有锁的对象上调用wait()方法（不是在线程上调用）。

3、线程未能获得对象锁（阻塞状态），它正试图运行该对象的同步方法或同步代码。

4、线程调度程序可以决定将当前运行状态移动到可运行状态，以便让另一个线程获得运行机会，而不需要任何理由。

## 锁类型

可重入锁：在执行对象中所有同步方法不用再次获得锁

可中断锁：在等待获取锁过程中可中断

公平锁： 按等待获取锁的线程的等待时间进行获取，等待时间长的具有优先获取锁权利

读写锁：对资源读取和写入的时候拆分为2部分处理，读的时候可以多线程一起读，写的时候必须同步地写

## 两种锁机制的底层的实现策略

**互斥同步**最主要的问题就是进行线程阻塞和唤醒所带来的性能问题，因而这种同步又称为阻塞同步，它属于一种悲观的并发策略，即线程获得的是独占锁。独占锁意味着其他线程只能依靠阻塞来等待线程释放锁。而在CPU转换线程阻塞时会引起线程上下文切换，当有很多线程竞争锁的时候，会引起CPU频繁的上下文切换导致效率很低。synchronized采用的便是这种并发策略。

  随着指令集的发展，我们有了另一种选择：基于冲突检测的**乐观并发策略**，通俗地讲就是先进性操作，如果没有其他线程争用共享数据，那操作就成功了，如果共享数据被争用，产生了冲突，那就再进行其他的补偿措施（最常见的补偿措施就是不断地重拾，直到试成功为止），这种乐观的并发策略的许多实现都不需要把线程挂起，因此这种同步被称为非阻塞同步。ReetrantLock采用的便是这种并发策略。

  在**乐观并发策略**中，需要操作和冲突检测这两个步骤具备原子性，它靠硬件指令来保证，这里用的是CAS操作（Compare and Swap）。JDK1.5之后，Java程序才可以使用CAS操作。我们可以进一步研究ReentrantLock的源代码，会发现其中比较重要的获得锁的一个方法是compareAndSetState，这里其实就是调用的CPU提供的特殊指令。现代的CPU提供了指令，可以自动更新共享数据，而且能够检测到其他线程的干扰，而compareAndSet() 就用这些代替了锁定。这个算法称作非阻塞算法，意思是一个线程的失败或者挂起不应该影响其他线程的失败或挂起。

  Java 5中引入了注入AutomicInteger、AutomicLong、AutomicReference等特殊的原子性变量类，它们提供的如：compareAndSet（）、incrementAndSet（）和getAndIncrement（）等方法都使用了CAS操作。因此，它们都是由硬件指令来保证的原子方法。

## Lock接口中主要有哪些方法

lock()：获取锁，如果锁被暂用则一直等待

unlock():释放锁

tryLock(): 注意返回类型是boolean，如果获取锁的时候锁被占用就返回false，否则返回true

tryLock(long time, TimeUnit unit)：比起tryLock()就是给了一个时间期限，保证等待参数时间

lockInterruptibly()：用该锁的获得方式，如果线程在获取锁的阶段进入了等待，那么可以中断此线程，先去做别的事

## Java锁Lock——ReentrantLock实现原理及源码分析

​    **Lock****完全用Java写成，在java这个层面是无关JVM实现的。在java.util.concurrent.locks包中有很多Lock的实现类，常用的有ReentrantLock、ReadWriteLock（实现类ReentrantReadWriteLock），其实现都依赖java.util.concurrent.AbstractQueuedSynchronizer类，实现思路都大同小异，因此我们以ReentrantLock作为讲解切入点。**

**ReentrantLock****是Java并发包中提供的一个可重入的互斥锁。ReentrantLock和synchronized在基本用法，行为语义上都是类似的，同样都具有可重入性。只不过相比原生的Synchronized，ReentrantLock增加了一些高级的扩展功能，比如它可以实现公平锁，同时也可以绑定多个Conditon。**

**ReentrantLock****是基于AQS的，AQS是基于CAS+CHL实现，它是Java并发包中众多同步组件的构建基础，它通过一个int类型的状态变量state和一个FIFO队列（CLH队列）来完成共享资源的获取，线程的排队等待等。AQS是个底层框架，采用模板方法模式，它定义了通用的较为复杂的逻辑骨架，比如线程的排队，阻塞，唤醒等，将这些复杂但实质通用的部分抽取出来，这些都是需要构建同步组件的使用者无需关心的，使用者仅需重写一些简单的指定的方法即可（其实就是对于共享变量state的一些简单的获取释放的操作）。**

l **ReentrantLock****的调用过程**

经过观察ReentrantLock把所有Lock接口的操作都委派到一个**Sync****类**上，该类继承了**AbstractQueuedSynchronizer**：

```
static abstract class Sync extends AbstractQueuedSynchronizer
```

Sync又有两个子类：

```
final static class NonfairSync extends Sync
final static class FairSync extends Sync
```

显然是为了支持公平锁和非公平锁而定义，默认情况下为非公平锁。

先理一下Reentrant.lock()方法的调用过程（默认非公平锁）：

![img](http://image.itstabber.com/2020-09-09/clip_image018.png)

这些讨厌的Template模式导致很难直观的看到整个调用过程，其实通过上面调用过程及AbstractQueuedSynchronizer的注释可以发现，AbstractQueuedSynchronizer中抽象了绝大多数Lock的功能，而只把tryAcquire方法延迟到子类中实现。tryAcquire方法的语义在于用具体子类判断请求线程是否可以获得锁，无论成功与否AbstractQueuedSynchronizer都将处理后面的流程。

l **CLH****队列**

AQS内部维护着一个FIFO的队列，即CLH队列。AQS的同步机制就是依靠CLH队列实现的。CLH队列是FIFO的双端双向队列，实现公平锁。线程通过AQS获取锁失败，就会将线程封装成一个Node节点，插入队列尾。当有线程释放锁时，后尝试把队头的next节点占用锁。**CLH****队列结构如下：**

![img](http://image.itstabber.com/2020-09-09/clip_image019.png)

CLH队列由Node对象组成，Node是AQS中的内部类。

l **锁实现（加锁）**

简单说来，AbstractQueuedSynchronizer会把所有的请求线程构成一个CLH队列，当一个线程执行完毕（lock.unlock()）时会激活自己的后继节点，但正在执行的线程并不在队列中，而那些等待执行的线程全部处于阻塞状态，经过调查线程的显式阻塞是通过调用LockSupport.park()完成，而LockSupport.park()则调用sun.misc.Unsafe.park()本地方法，再进一步，HotSpot在Linux中中通过调用pthread_mutex_lock函数把线程交给系统内核进行阻塞。该队列如图：

p(predecessor) 前任，前辈，前身；mutex：n，互斥、互斥锁、互斥体、互斥元、互斥量

![img](http://image.itstabber.com/2020-09-09/clip_image020.png)

与synchronized相同的是，这也是一个虚拟队列，不存在队列实例，仅存在节点之间的前后关系。令人疑惑的是为什么采用CLH队列呢？原生的CLH队列是用于自旋锁，但Doug Lea把其改造为阻塞锁。
 **当有线程竞争锁时，该线程会首先尝试获得锁，这对于那些已经在队列中排队的线程来说显得不公平，这也是非公平锁的由来**，与synchronized实现类似，这样会极大提高吞吐量。

如果已经存在Running线程，则新的竞争线程会被追加到队尾，具体是采用基于CAS（Compare and Swap）的Lock-Free算法，因为线程并发对Tail调用CAS可能会导致其他线程CAS失败，解决办法是**循环CAS直至成功**。AbstractQueuedSynchronizer的实现非常精巧，令人叹为观止，不入细节难以完全领会其精髓，下面详细说明实现过程：

l **1 Sync.nonfairTryAcquire**

nonfairTryAcquire方法将是lock方法间接调用的第一个方法，每次请求锁时都会首先调用该方法。

l **2 AbstractQueuedSynchronizer.addWaiter**

addWaiter方法负责把**当前无法获得锁的线程包装为一个Node添加到队尾**

l **3 AbstractQueuedSynchronizer.acquireQueued**

acquireQueued的主要作用是把已经追加到队列的线程节点（addWaiter方法返回值）进行阻塞，但阻塞前又通过tryAccquire重试是否能获得锁，如果重试成功能则无需阻塞，直接返回。

LockSupport.park最终把线程交给系统（Linux）内核进行阻塞。当然也不是马上把请求不到锁的线程进行阻塞，还要检查该线程的状态。

l **解锁**

​    解锁代码相对简单，主要体现在AbstractQueuedSynchronizer.release和Sync.tryRelease方法。tryRelease与tryAcquire语义相同，把如何释放的逻辑延迟到子类中。tryRelease语义很明确：如果线程多次锁定，则进行多次释放，直至status==0则真正释放锁，所谓释放锁即设置status为0，因为无竞争所以没有使用CAS。release的语义在于：如果可以释放锁，则唤醒队列第一个线程（Head），最后通知系统内核继续该线程，在Linux下是通过pthread_mutex_unlock完成。

p(predecessor) 前任，前辈，前身；mutex：n，互斥、互斥锁、互斥体、互斥元、互斥量







